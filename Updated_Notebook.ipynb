{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9tRbTa8mGCq1"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/atikul-islam-sajib/TreeBasedModel.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd TreeBasedModel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJh802yDGeTs",
        "outputId": "6b60f8d1-683c-4cbd-d062-1c561c7a04eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TreeBasedModel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "tt8TH5ktGzAZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "id": "SpwU6Wp0G45p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task ?\n",
        "\n",
        "specified that I do not intend to optimize the parameter \\( k \\) this time, but rather to document the results for each value of \\( k \\). Specifically, goal is to visualize importance measures—MDI and SHAP included—as functions of \\( k \\), while keeping the `max_features` parameter at its default setting. There is no need to optimize for ROC AUC scores; however, want these scores recorded for each \\( k \\). Furthermore, for each level of feature relevance, aim to produce a set of four plots that illustrate these metrics."
      ],
      "metadata": {
        "id": "3whZNlXgetcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With K and Max Feature = Default - Custom RF"
      ],
      "metadata": {
        "id": "QcP1Q-XG7oPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from TreeModelsFromScratch.RandomForest import RandomForest\n",
        "from utils import simulate_data_strobl\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def calculate_auc_roc(y_true, y_scores):\n",
        "    \"\"\"Calculate the AUC-ROC score.\"\"\"\n",
        "    return roc_auc_score(y_true, y_scores)\n",
        "\n",
        "def evaluate_models(X_train, y_train, X_test, y_test, k_values):\n",
        "    \"\"\"Evaluate RandomForest models for a range of k values and record MDI, SHAP, and ROC AUC.\"\"\"\n",
        "    results = []\n",
        "    for k in k_values:\n",
        "        model = RandomForest(treetype=\"classification\", n_trees=25, k=k, random_state=42, oob_SHAP=True)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_scores = model.predict_proba(X_test)[:, 1]\n",
        "        roc_auc = calculate_auc_roc(y_test, y_scores)\n",
        "        mdi_importances = model.feature_importances_\n",
        "        shap_values = np.mean(model.oob_SHAP_values, axis=0)  # Average SHAP importance per feature\n",
        "\n",
        "        results.append({\n",
        "            'k': k,\n",
        "            'roc_auc': roc_auc,\n",
        "            'mdi_importances': mdi_importances,\n",
        "            'shap_values': shap_values\n",
        "        })\n",
        "    return results\n",
        "\n",
        "relevance_values = [0, 0.05, 0.1, 0.15, 0.2]\n",
        "k_values = range(1, 31)\n",
        "all_results = []\n",
        "\n",
        "for relevance in relevance_values:\n",
        "    X, y = simulate_data_strobl(n=300, relevance=relevance, seed=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    results = evaluate_models(X_train, y_train, X_test, y_test, k_values)\n",
        "    all_results.append({'relevance': relevance, 'results': results})\n",
        "\n",
        "for relevance_result in all_results:\n",
        "    relevance = relevance_result['relevance']\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    ks = [result['k'] for result in relevance_result['results']]\n",
        "    rocs = [result['roc_auc'] for result in relevance_result['results']]\n",
        "    mdi_means = [result['mdi_importances'] for result in relevance_result['results']]\n",
        "    shap_means = [result['shap_values'] for result in relevance_result['results']]\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(ks, rocs, label=f'Relevance {relevance:.2f}', marker = \"o\")\n",
        "    plt.title('ROC AUC vs k')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('ROC AUC')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(ks, mdi_means, label=f'MDI - Relevance {relevance:.2f}', marker = \"o\")\n",
        "    plt.title('MDI vs k')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('MDI Importance')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(ks, shap_means, label=f'SHAP - Relevance {relevance:.2f}', marker = \"o\")\n",
        "    plt.title('SHAP vs k')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('SHAP Importance')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MnNsZ1Gya3P2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With Depth and K - > depth = True"
      ],
      "metadata": {
        "id": "cxQPnvML8iV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from TreeModelsFromScratch.RandomForest import RandomForest\n",
        "from utils import simulate_data_strobl\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def calculate_auc_roc(y_true, y_scores):\n",
        "    \"\"\"Calculate the AUC-ROC score.\"\"\"\n",
        "    return roc_auc_score(y_true, y_scores)\n",
        "\n",
        "def evaluate_models(X_train, y_train, X_test, y_test, k_values):\n",
        "    \"\"\"Evaluate RandomForest models for a range of k values and record MDI, SHAP, and ROC AUC.\"\"\"\n",
        "    results = []\n",
        "    for k in k_values:\n",
        "        model = RandomForest(treetype=\"classification\", n_trees=25, k=k, random_state=42, oob_SHAP=True, depth_dof=True)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_scores = model.predict_proba(X_test)[:, 1]\n",
        "        roc_auc = calculate_auc_roc(y_test, y_scores)\n",
        "        mdi_importances = model.feature_importances_\n",
        "        shap_values = np.mean(model.oob_SHAP_values, axis=0)  # Average SHAP importance per feature\n",
        "\n",
        "        results.append({\n",
        "            'k': k,\n",
        "            'roc_auc': roc_auc,\n",
        "            'mdi_importances': mdi_importances,\n",
        "            'shap_values': shap_values\n",
        "        })\n",
        "    return results\n",
        "\n",
        "relevance_values = [0, 0.05, 0.1, 0.15, 0.2]\n",
        "k_values = range(1, 31)\n",
        "all_results = []\n",
        "\n",
        "for relevance in relevance_values:\n",
        "    X, y = simulate_data_strobl(n=300, relevance=relevance, seed=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    results = evaluate_models(X_train, y_train, X_test, y_test, k_values)\n",
        "    all_results.append({'relevance': relevance, 'results': results})\n",
        "\n",
        "for relevance_result in all_results:\n",
        "    relevance = relevance_result['relevance']\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    ks = [result['k'] for result in relevance_result['results']]\n",
        "    rocs = [result['roc_auc'] for result in relevance_result['results']]\n",
        "    mdi_means = [result['mdi_importances'] for result in relevance_result['results']]\n",
        "    shap_means = [result['shap_values'] for result in relevance_result['results']]\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(ks, rocs, label=f'Relevance {relevance:.2f}', marker = \"o\")\n",
        "    plt.title('ROC AUC vs k')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('ROC AUC')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(ks, mdi_means, label=f'MDI - Relevance {relevance:.2f}', marker = \"o\")\n",
        "    plt.title('MDI vs k')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('MDI Importance')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(ks, shap_means, label=f'SHAP - Relevance {relevance:.2f}', marker = \"o\")\n",
        "    plt.title('SHAP vs k')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('SHAP Importance')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TxCs5oIlOkvl"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}