{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.utils import check_random_state\n",
        "import numpy as np\n",
        "\n",
        "def generate_sample_indices(random_state, n_samples):\n",
        "    \"\"\"Generate bootstrap sample indices for in-bag samples.\"\"\"\n",
        "    random_instance = check_random_state(random_state)\n",
        "    sample_indices = random_instance.randint(0, n_samples, n_samples)\n",
        "    return sample_indices\n",
        "\n",
        "def generate_unsampled_indices(random_state, n_samples):\n",
        "    \"\"\"Generate indices for out-of-bag (OOB) samples.\"\"\"\n",
        "    sample_indices = generate_sample_indices(random_state, n_samples)\n",
        "    sample_counts = np.bincount(sample_indices, minlength=n_samples)\n",
        "    unsampled_mask = sample_counts == 0\n",
        "    indices_range = np.arange(n_samples)\n",
        "    unsampled_indices = indices_range[unsampled_mask]\n",
        "    return unsampled_indices\n",
        "\n",
        "class CustomRandomForestClassifier(RandomForestClassifier):\n",
        "    def fit(self, X, y):\n",
        "        # Fit the model using the super class\n",
        "        super().fit(X, y)\n",
        "\n",
        "        # Initialize lists to store in-bag, OOB indices, and OOB losses for each tree\n",
        "        self.in_bag_indices_ = []\n",
        "        self.oob_indices_ = []\n",
        "        self.tree_weights_ = []\n",
        "\n",
        "        for estimator in self.estimators_:\n",
        "            # Generate in-bag and OOB indices\n",
        "            random_state = estimator.random_state\n",
        "            in_bag_indices = generate_sample_indices(random_state, len(X))\n",
        "            oob_indices = generate_unsampled_indices(random_state, len(X))\n",
        "\n",
        "            # Store in-bag and OOB indices\n",
        "            self.in_bag_indices_.append(in_bag_indices)\n",
        "            self.oob_indices_.append(oob_indices)\n",
        "\n",
        "            # Calculate and store OOB loss\n",
        "            if len(oob_indices) > 0:\n",
        "                oob_predictions = estimator.predict(X[oob_indices])\n",
        "                oob_loss = mean_squared_error(y[oob_indices], oob_predictions)\n",
        "                self.tree_weights_.append(np.exp(-oob_loss))\n",
        "            else:\n",
        "                self.tree_weights_.append(0)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Check if forest is fitted\n",
        "        if not hasattr(self, \"estimators_\"):\n",
        "            raise ValueError(\"The forest is not fitted yet!\")\n",
        "\n",
        "        # Aggregate predictions from all trees, weighted by their OOB loss-based weights\n",
        "        weighted_preds = np.zeros((X.shape[0], len(self.classes_)))\n",
        "        for tree, weight in zip(self.estimators_, self.tree_weights_):\n",
        "            preds = tree.predict_proba(X)\n",
        "            weighted_preds += weight * preds\n",
        "\n",
        "        final_preds = np.argmax(weighted_preds, axis=1)\n",
        "        return self.classes_[final_preds]\n",
        "\n",
        "# Example usage\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "clf = CustomRandomForestClassifier(oob_score=True)\n",
        "clf.fit(X, y)\n",
        "predictions = clf.predict(X)\n",
        "\n",
        "# Accessing in-bag and OOB data for each tree\n",
        "for i, tree in enumerate(clf.estimators_):\n",
        "    in_bag_samples_X = X[clf.in_bag_indices_[i]]\n",
        "    in_bag_samples_y = y[clf.in_bag_indices_[i]]\n",
        "    oob_samples_X = X[clf.oob_indices_[i]]\n",
        "    oob_samples_y = y[clf.oob_indices_[i]]"
      ],
      "metadata": {
        "id": "AI5qF2UQpWGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.utils import check_random_state\n",
        "import numpy as np\n",
        "\n",
        "# Function to generate indices for random samples from a dataset\n",
        "def generate_sample_indices(random_state, n_samples):\n",
        "    \"\"\"\n",
        "    Generate random indices for selecting samples from a dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - random_state: A random number generator.\n",
        "    - n_samples: The total number of samples in the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - sample_indices: An array of random sample indices.\n",
        "    \"\"\"\n",
        "    random_instance = check_random_state(random_state)\n",
        "    sample_indices = random_instance.randint(0, n_samples, n_samples)\n",
        "    return sample_indices\n",
        "\n",
        "# Function to generate indices for samples that are not selected (out-of-bag samples)\n",
        "def generate_unsampled_indices(random_state, n_samples):\n",
        "    \"\"\"\n",
        "    Generate indices for samples that are not selected (out-of-bag samples).\n",
        "\n",
        "    Parameters:\n",
        "    - random_state: A random number generator.\n",
        "    - n_samples: The total number of samples in the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - unsampled_indices: An array of indices representing out-of-bag samples.\n",
        "    \"\"\"\n",
        "    sample_indices = generate_sample_indices(random_state, n_samples)\n",
        "    sample_counts = np.bincount(sample_indices, minlength=n_samples)\n",
        "    unsampled_mask = sample_counts == 0\n",
        "    indices_range = np.arange(n_samples)\n",
        "    unsampled_indices = indices_range[unsampled_mask]\n",
        "    return unsampled_indices\n",
        "\n",
        "# Custom RandomForestClassifier class\n",
        "class CustomRandomForestClassifier(RandomForestClassifier):\n",
        "    \"\"\"\n",
        "    A custom implementation of RandomForestClassifier with additional features.\n",
        "\n",
        "    This class extends the functionality of the RandomForestClassifier from scikit-learn.\n",
        "\n",
        "    Methods:\n",
        "    - fit(X, y): Fit the model to the training data.\n",
        "    - predict(X): Make predictions using the fitted model.\n",
        "\n",
        "    Attributes:\n",
        "    - in_bag_indices_: A list of indices representing samples used for training in each tree.\n",
        "    - oob_indices_: A list of indices representing out-of-bag samples for each tree.\n",
        "    - tree_weights_: A list of weights assigned to each tree based on its performance.\n",
        "\n",
        "    Example Usage:\n",
        "    - X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "    - clf = CustomRandomForestClassifier(oob_score=True)\n",
        "    - clf.fit(X, y)\n",
        "    - predictions = clf.predict(X)\n",
        "    \"\"\"\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the custom random forest model to the training data.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features (numpy array or pandas DataFrame).\n",
        "        - y: Target labels (numpy array or pandas Series).\n",
        "\n",
        "        Returns:\n",
        "        - self: The fitted model.\n",
        "        \"\"\"\n",
        "        # Fit the model using the superclass RandomForestClassifier\n",
        "        super().fit(X, y)\n",
        "\n",
        "        # Initialize lists to store in-bag, out-of-bag indices, and out-of-bag loss weights for each tree\n",
        "        self.in_bag_indices_ = []\n",
        "        self.oob_indices_ = []\n",
        "        self.tree_weights_ = []\n",
        "\n",
        "        for estimator in self.estimators_:\n",
        "            # Generate in-bag and out-of-bag indices for each tree\n",
        "            random_state = estimator.random_state\n",
        "            in_bag_indices = generate_sample_indices(random_state, len(X))\n",
        "            oob_indices = generate_unsampled_indices(random_state, len(X))\n",
        "\n",
        "            # Store in-bag and out-of-bag indices\n",
        "            self.in_bag_indices_.append(in_bag_indices)\n",
        "            self.oob_indices_.append(oob_indices)\n",
        "\n",
        "            # Calculate and store out-of-bag loss-based weights\n",
        "            if len(oob_indices) > 0:\n",
        "                oob_predictions = estimator.predict(X[oob_indices])\n",
        "                oob_loss = mean_squared_error(y[oob_indices], oob_predictions)\n",
        "                self.tree_weights_.append(np.exp(-oob_loss))\n",
        "            else:\n",
        "                self.tree_weights_.append(0)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the fitted custom random forest model.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features for making predictions.\n",
        "\n",
        "        Returns:\n",
        "        - final_preds: Predicted class labels.\n",
        "        \"\"\"\n",
        "        # Check if the forest is fitted\n",
        "        if not hasattr(self, \"estimators_\"):\n",
        "            raise ValueError(\"The forest is not fitted yet!\")\n",
        "\n",
        "        # Aggregate predictions from all trees, weighted by their out-of-bag loss-based weights\n",
        "        weighted_preds = np.zeros((X.shape[0], len(self.classes_)))\n",
        "        for tree, weight in zip(self.estimators_, self.tree_weights_):\n",
        "            preds = tree.predict_proba(X)\n",
        "            weighted_preds += weight * preds\n",
        "\n",
        "        final_preds = np.argmax(weighted_preds, axis=1)\n",
        "        return self.classes_[final_preds]\n",
        "\n",
        "# Example usage\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "clf = CustomRandomForestClassifier(oob_score=True)\n",
        "clf.fit(X, y)\n",
        "predictions = clf.predict(X)\n",
        "\n",
        "# Accessing in-bag and out-of-bag data for each tree\n",
        "for i, tree in enumerate(clf.estimators_):\n",
        "    in_bag_samples_X = X[clf.in_bag_indices_[i]]\n",
        "    in_bag_samples_y = y[clf.in_bag_indices_[i]]\n",
        "    oob_samples_X = X[clf.oob_indices_[i]]\n",
        "    oob_samples_y = y[clf.oob_indices_[i]]"
      ],
      "metadata": {
        "id": "wir8V_qup25w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The updated one\n",
        "And since I am at it, I have two feature requests:\n",
        "I would like these functions/classes to become part of a library/module which we can easily reuse in the future. (by importing a module)\n",
        "As I mentioned, I would like to try out different weighting schemes, exp(-L_oob) is just a special case. So maybe we can add a parameter to the predict function (such as def predict(self, X, weights = \"expOOB\")) which can then take on different values in the future ?"
      ],
      "metadata": {
        "id": "9zU8pHoc69h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# Function to generate indices for random samples from a dataset\n",
        "def generate_sample_indices(random_state, n_samples):\n",
        "    \"\"\"\n",
        "    Generate random indices for selecting samples from a dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - random_state: A random number generator.\n",
        "    - n_samples: The total number of samples in the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - sample_indices: An array of random sample indices.\n",
        "    \"\"\"\n",
        "    random_instance = check_random_state(random_state)\n",
        "    sample_indices = random_instance.randint(0, n_samples, n_samples)\n",
        "    return sample_indices\n",
        "\n",
        "# Function to generate indices for samples that are not selected (out-of-bag samples)\n",
        "def generate_unsampled_indices(random_state, n_samples):\n",
        "    \"\"\"\n",
        "    Generate indices for samples that are not selected (out-of-bag samples).\n",
        "\n",
        "    Parameters:\n",
        "    - random_state: A random number generator.\n",
        "    - n_samples: The total number of samples in the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - unsampled_indices: An array of indices representing out-of-bag samples.\n",
        "    \"\"\"\n",
        "    sample_indices = generate_sample_indices(random_state, n_samples)\n",
        "    sample_counts = np.bincount(sample_indices, minlength=n_samples)\n",
        "    unsampled_mask = sample_counts == 0\n",
        "    indices_range = np.arange(n_samples)\n",
        "    unsampled_indices = indices_range[unsampled_mask]\n",
        "    return unsampled_indices\n",
        "\n",
        "# Custom RandomForestClassifier class\n",
        "class CustomRandomForestClassifier(RandomForestClassifier):\n",
        "    \"\"\"\n",
        "      A custom implementation of RandomForestClassifier from scikit-learn with additional\n",
        "      features for handling in-bag and out-of-bag samples and customizable prediction weighting schemes.\n",
        "\n",
        "      This class extends RandomForestClassifier and provides detailed tracking of sample indices used\n",
        "      for training each tree (in-bag) and those not selected (out-of-bag). It also allows for different\n",
        "      weighting schemes when making predictions.\n",
        "\n",
        "      Methods:\n",
        "      - fit(X, y): Fit the model to the training data.\n",
        "      - predict(X, weights): Make predictions using the fitted model with a specified weighting scheme.\n",
        "\n",
        "      Parameters:\n",
        "      - oob_score (bool): Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
        "\n",
        "      Attributes:\n",
        "      - in_bag_indices_ (list): A list where each element is an array of indices representing samples\n",
        "        used for training each tree.\n",
        "      - oob_indices_ (list): A list where each element is an array of indices representing out-of-bag\n",
        "        samples for each tree.\n",
        "      - tree_weights_ (list): A list of weights for each tree, calculated based on the out-of-bag loss\n",
        "        for each tree if the 'expOOB' weighting scheme is used.\n",
        "\n",
        "      Example Usage:\n",
        "          from sklearn.datasets import make_classification\n",
        "\n",
        "          X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "          clf = CustomRandomForestClassifier(oob_score=True)\n",
        "          clf.fit(X, y)\n",
        "\n",
        "          # Predict using different weighting schemes\n",
        "          predictions_default = clf.predict(X)  # Defaults to 'uniform' weighting\n",
        "          predictions_expOOB = clf.predict(X, weights=\"expOOB\")\n",
        "          predictions_uniform = clf.predict(X, weights=\"uniform\")\n",
        "    \"\"\"\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the custom random forest model to the training data.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features (numpy array or pandas DataFrame).\n",
        "        - y: Target labels (numpy array or pandas Series).\n",
        "\n",
        "        Returns:\n",
        "        - self: The fitted model instance.\n",
        "        \"\"\"\n",
        "    def fit(self, X, y):\n",
        "        super().fit(X, y)\n",
        "        self.in_bag_indices_ = []\n",
        "        self.oob_indices_ = []\n",
        "        self.tree_weights_ = []\n",
        "\n",
        "        for estimator in self.estimators_:\n",
        "            random_state = estimator.random_state\n",
        "            in_bag_indices = generate_sample_indices(random_state, len(X))\n",
        "            oob_indices = generate_unsampled_indices(random_state, len(X))\n",
        "\n",
        "            self.in_bag_indices_.append(in_bag_indices)\n",
        "            self.oob_indices_.append(oob_indices)\n",
        "\n",
        "            if len(oob_indices) > 0:\n",
        "                oob_predictions = estimator.predict(X[oob_indices])\n",
        "                oob_loss = mean_squared_error(y[oob_indices], oob_predictions)\n",
        "                self.tree_weights_.append(np.exp(-oob_loss))\n",
        "            else:\n",
        "                self.tree_weights_.append(0)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X, weights=None):\n",
        "        \"\"\"\n",
        "        Make predictions using the fitted custom random forest model.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features for making predictions (numpy array or pandas DataFrame).\n",
        "        - weights (optional): The weighting scheme to use for aggregating predictions. Supported values:\n",
        "          \"expOOB\" (weights based on the exponential of the negative out-of-bag error) and \"uniform\"\n",
        "          (equal weighting). Defaults to \"uniform\" if not specified or if an unknown value is passed.\n",
        "\n",
        "        Returns:\n",
        "        - final_preds: An array of predicted class labels.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"estimators_\"):\n",
        "            raise ValueError(\"The forest is not fitted yet!\")\n",
        "\n",
        "        weighted_preds = np.zeros((X.shape[0], len(self.classes_)))\n",
        "\n",
        "        if weights is None or weights not in [\"expOOB\", \"uniform\"]:\n",
        "            weights = \"uniform\"\n",
        "\n",
        "        if weights == \"expOOB\":\n",
        "            for tree, weight in zip(self.estimators_, self.tree_weights_):\n",
        "                preds = tree.predict_proba(X)\n",
        "                weighted_preds += weight * preds\n",
        "        elif weights == \"uniform\":\n",
        "            for tree in self.estimators_:\n",
        "                preds = tree.predict_proba(X)\n",
        "                weighted_preds += preds / len(self.estimators_)\n",
        "\n",
        "        final_preds = np.argmax(weighted_preds, axis=1)\n",
        "        return self.classes_[final_preds]\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    from sklearn.datasets import make_classification\n",
        "\n",
        "    X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "    clf = CustomRandomForestClassifier(oob_score=True)\n",
        "    clf.fit(X, y)\n",
        "    predictions_default = clf.predict(X)\n",
        "    predictions_expOOB = clf.predict(X, weights=\"expOOB\")\n",
        "    predictions_uniform = clf.predict(X, weights=\"uniform\")\n",
        "\n",
        "    print(\"Default Predictions:\", predictions_default)\n",
        "    print(\"ExpOOB Predictions:\", predictions_expOOB)\n",
        "    print(\"Uniform Predictions:\", predictions_uniform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZsbCXeM6-id",
        "outputId": "0ac58267-aefd-45ea-e59c-81aa96313843"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default Predictions: [1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1\n",
            " 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 0]\n",
            "ExpOOB Predictions: [1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1\n",
            " 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 0]\n",
            "Uniform Predictions: [1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1\n",
            " 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imodels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLXAysJh_rcf",
        "outputId": "afeb98cf-a47b-496d-bc30-b1172bb95660"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imodels\n",
            "  Downloading imodels-1.4.1-py3-none-any.whl (231 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.2/231.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imodels) (3.7.1)\n",
            "Requirement already satisfied: mlxtend>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from imodels) (0.22.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imodels) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from imodels) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from imodels) (2.31.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imodels) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from imodels) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from imodels) (4.66.1)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend>=0.18.0->imodels) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from mlxtend>=0.18.0->imodels) (67.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->imodels) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->imodels) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->imodels) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->imodels) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->imodels) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->imodels) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->imodels) (1.16.0)\n",
            "Installing collected packages: imodels\n",
            "Successfully installed imodels-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bechmarking Data"
      ],
      "metadata": {
        "id": "PkHKp4KL_IwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pmlb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-BW-KzY_vGW",
        "outputId": "fa8fad1e-ab1e-46ff-9a83-802ece31c6d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pmlb\n",
            "  Downloading pmlb-1.0.1.post3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from pmlb) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from pmlb) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from pmlb) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pmlb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pmlb) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pmlb) (1.23.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->pmlb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->pmlb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->pmlb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->pmlb) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.5->pmlb) (1.16.0)\n",
            "Installing collected packages: pmlb\n",
            "Successfully installed pmlb-1.0.1.post3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imodels.util.data_util import get_clean_dataset\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def evaluate_datasets(datasets, random_state=42):\n",
        "    \"\"\"\n",
        "    Evaluate multiple datasets using a custom random forest classifier.\n",
        "\n",
        "    Parameters:\n",
        "    - datasets (list): A list of dataset names.\n",
        "    - random_state (int): A seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - roc_auc_scores_default (list): ROC AUC scores for the default weighting scheme.\n",
        "    - roc_auc_scores_expOOB (list): ROC AUC scores for the expOOB weighting scheme.\n",
        "    \"\"\"\n",
        "    roc_auc_scores_default = []\n",
        "    roc_auc_scores_expOOB = []\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        # Fetch the dataset\n",
        "        X, y, feature_names = get_clean_dataset(dataset_name)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_state)\n",
        "\n",
        "        # Initialize and train the model\n",
        "        clf = CustomRandomForestClassifier(oob_score=True, random_state=random_state)\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions_default = clf.predict(X_test, weights=\"uniform\")\n",
        "        predictions_expOOB = clf.predict(X_test, weights=\"expOOB\")\n",
        "\n",
        "        # Calculate ROC AUC scores\n",
        "        roc_auc_default = roc_auc_score(y_test, predictions_default)\n",
        "        roc_auc_expOOB = roc_auc_score(y_test, predictions_expOOB)\n",
        "\n",
        "        # Store the ROC AUC scores\n",
        "        roc_auc_scores_default.append((dataset_name, roc_auc_default))\n",
        "        roc_auc_scores_expOOB.append((dataset_name, roc_auc_expOOB))\n",
        "\n",
        "    return roc_auc_scores_default, roc_auc_scores_expOOB\n",
        "\n",
        "# Datasets to evaluate\n",
        "dataset_names = [\"diabetes\", \"breast_cancer\", \"heart\", \"haberman\", \"fico\", \"enhancer\", \"credit_g\", \"juvenile_clean\"]\n",
        "\n",
        "# Evaluate datasets\n",
        "roc_auc_scores_default, roc_auc_scores_expOOB = evaluate_datasets(dataset_names)\n",
        "\n",
        "# Print ROC AUC scores for each dataset\n",
        "for dataset_name, score in roc_auc_scores_default:\n",
        "    print(f'Default ROC AUC for {dataset_name}: {score}')\n",
        "\n",
        "print(\"=\"*100)\n",
        "\n",
        "for dataset_name, score in roc_auc_scores_expOOB:\n",
        "    print(f'expOOB ROC AUC for {dataset_name}: {score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ74wGbV_Kji",
        "outputId": "05d65221-de48-4353-dfd1-39e44120a52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fetching diabetes from pmlb\n",
            "fetching heart from imodels\n",
            "fetching fico from imodels\n",
            "fetching credit_g from imodels\n",
            "Default ROC AUC for diabetes: 0.7255555555555555\n",
            "Default ROC AUC for breast_cancer: 0.7142857142857144\n",
            "Default ROC AUC for heart: 0.8203463203463204\n",
            "Default ROC AUC for haberman: 0.5656565656565656\n",
            "Default ROC AUC for fico: 0.6987820775720646\n",
            "Default ROC AUC for enhancer: 0.6695426457107301\n",
            "Default ROC AUC for credit_g: 0.7053732419761991\n",
            "Default ROC AUC for juvenile_clean: 0.729199372056515\n",
            "====================================================================================================\n",
            "expOOB ROC AUC for diabetes: 0.7298148148148148\n",
            "expOOB ROC AUC for breast_cancer: 0.7142857142857144\n",
            "expOOB ROC AUC for heart: 0.8441558441558441\n",
            "expOOB ROC AUC for haberman: 0.5656565656565656\n",
            "expOOB ROC AUC for fico: 0.6990878398172019\n",
            "expOOB ROC AUC for enhancer: 0.6728983504087167\n",
            "expOOB ROC AUC for credit_g: 0.6714749368914533\n",
            "expOOB ROC AUC for juvenile_clean: 0.7401883830455258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#With probability - (Professor Desired Code)\n",
        "\n",
        "It looks pretty good, just one thing has to be changed.\n",
        "I had mentioned this in my message on slack last Monday:\n",
        "\"you should not do the argmax computation in the end, instead it should return probabilities, so we can compute AUC-ROC values\"\n",
        "To get proper AUC scores one should pass the probabilities, not the thresholded 0/1 labels"
      ],
      "metadata": {
        "id": "jYZgiUy8EMLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# Function to generate indices for random samples from a dataset\n",
        "def generate_sample_indices(random_state, n_samples):\n",
        "    random_instance = check_random_state(random_state)\n",
        "    sample_indices = random_instance.randint(0, n_samples, n_samples)\n",
        "    return sample_indices\n",
        "\n",
        "# Function to generate indices for samples that are not selected (out-of-bag samples)\n",
        "def generate_unsampled_indices(random_state, n_samples):\n",
        "    sample_indices = generate_sample_indices(random_state, n_samples)\n",
        "    sample_counts = np.bincount(sample_indices, minlength=n_samples)\n",
        "    unsampled_mask = sample_counts == 0\n",
        "    indices_range = np.arange(n_samples)\n",
        "    unsampled_indices = indices_range[unsampled_mask]\n",
        "    return unsampled_indices\n",
        "\n",
        "# Custom RandomForestClassifier class\n",
        "class CustomRandomForestClassifier(RandomForestClassifier):\n",
        "    def fit(self, X, y):\n",
        "        super().fit(X, y)\n",
        "        self.in_bag_indices_ = []\n",
        "        self.oob_indices_ = []\n",
        "        self.tree_weights_ = []\n",
        "\n",
        "        for estimator in self.estimators_:\n",
        "            random_state = estimator.random_state\n",
        "            in_bag_indices = generate_sample_indices(random_state, len(X))\n",
        "            oob_indices = generate_unsampled_indices(random_state, len(X))\n",
        "\n",
        "            self.in_bag_indices_.append(in_bag_indices)\n",
        "            self.oob_indices_.append(oob_indices)\n",
        "\n",
        "            if len(oob_indices) > 0:\n",
        "                oob_predictions = estimator.predict(X[oob_indices])\n",
        "                oob_loss = mean_squared_error(y[oob_indices], oob_predictions)\n",
        "                self.tree_weights_.append(np.exp(-oob_loss))\n",
        "            else:\n",
        "                self.tree_weights_.append(0)\n",
        "\n",
        "        # Normalize tree weights\n",
        "        total_weight = np.sum(self.tree_weights_)\n",
        "        if total_weight > 0:\n",
        "            self.tree_weights_ = [weight / total_weight for weight in self.tree_weights_]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X, weights=None):\n",
        "        \"\"\"\n",
        "        Make predictions using the fitted custom random forest model.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features for making predictions (numpy array or pandas DataFrame).\n",
        "        - weights (optional): The weighting scheme to use for aggregating predictions. Supported values:\n",
        "          \"expOOB\" (weights based on the exponential of the negative out-of-bag error) and \"uniform\"\n",
        "          (equal weighting). Defaults to \"uniform\" if not specified or if an unknown value is passed.\n",
        "\n",
        "        Returns:\n",
        "        - final_preds: An array of predicted class labels.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"estimators_\"):\n",
        "            raise ValueError(\"The forest is not fitted yet!\")\n",
        "\n",
        "        weighted_preds = np.zeros((X.shape[0], len(self.classes_)))\n",
        "\n",
        "        if weights is None or weights not in [\"expOOB\", \"uniform\"]:\n",
        "            weights = \"uniform\"\n",
        "\n",
        "        if weights == \"expOOB\":\n",
        "            for tree, weight in zip(self.estimators_, self.tree_weights_):\n",
        "                preds = tree.predict_proba(X)\n",
        "                weighted_preds += weight * preds\n",
        "        elif weights == \"uniform\":\n",
        "            for tree in self.estimators_:\n",
        "                preds = tree.predict_proba(X)\n",
        "                weighted_preds += preds / len(self.estimators_)\n",
        "\n",
        "        final_preds = np.argmax(weighted_preds, axis=1)\n",
        "        return self.classes_[final_preds]\n",
        "\n",
        "\n",
        "    def predict_proba(self, X, weights=None):\n",
        "        if not hasattr(self, \"estimators_\"):\n",
        "            raise ValueError(\"The forest is not fitted yet!\")\n",
        "\n",
        "        weighted_preds = np.zeros((X.shape[0], len(self.classes_)))\n",
        "\n",
        "        if weights is None or weights not in [\"expOOB\", \"uniform\"]:\n",
        "            weights = \"uniform\"\n",
        "\n",
        "        if weights == \"expOOB\":\n",
        "            for tree, weight in zip(self.estimators_, self.tree_weights_):\n",
        "                preds = tree.predict_proba(X)\n",
        "                weighted_preds += weight * preds\n",
        "        elif weights == \"uniform\":\n",
        "            for tree in self.estimators_:\n",
        "                preds = tree.predict_proba(X)\n",
        "                weighted_preds += preds / len(self.estimators_)\n",
        "\n",
        "        return weighted_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgU8ujsBEyIQ",
        "outputId": "ba4cac99-413b-4d49-91bd-e88963e797df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imodels.util.data_util import get_clean_dataset\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def evaluate_datasets(datasets, random_state=42):\n",
        "    roc_auc_scores_default = []\n",
        "    roc_auc_scores_expOOB = []\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        # Fetch the dataset\n",
        "        X, y, feature_names = get_clean_dataset(dataset_name)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_state)\n",
        "\n",
        "        # Initialize and train the model\n",
        "        clf = CustomRandomForestClassifier(oob_score=True, random_state=random_state)\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions (probabilities)\n",
        "        # Assuming the positive class is labeled as '1' and is the second column\n",
        "        probabilities_default = clf.predict_proba(X_test, weights=\"uniform\")[:, 1]\n",
        "        probabilities_expOOB = clf.predict_proba(X_test, weights=\"expOOB\")[:, 1]\n",
        "\n",
        "        # Calculate ROC AUC scores\n",
        "        roc_auc_default = roc_auc_score(y_test, probabilities_default)\n",
        "        roc_auc_expOOB = roc_auc_score(y_test, probabilities_expOOB)\n",
        "\n",
        "        # Store the ROC AUC scores\n",
        "        roc_auc_scores_default.append((dataset_name, roc_auc_default))\n",
        "        roc_auc_scores_expOOB.append((dataset_name, roc_auc_expOOB))\n",
        "\n",
        "    return roc_auc_scores_default, roc_auc_scores_expOOB\n",
        "\n",
        "# Datasets to evaluate\n",
        "dataset_names = [\"diabetes\", \"breast_cancer\", \"heart\", \"haberman\", \"fico\", \"enhancer\", \"credit_g\", \"juvenile_clean\"]\n",
        "\n",
        "# Evaluate datasets\n",
        "roc_auc_scores_default, roc_auc_scores_expOOB = evaluate_datasets(dataset_names)\n",
        "\n",
        "# Print ROC AUC scores for each dataset\n",
        "for dataset_name, score in roc_auc_scores_default:\n",
        "    print(f'Default ROC AUC for {dataset_name}: {score}')\n",
        "\n",
        "print(\"=\"*100)\n",
        "\n",
        "for dataset_name, score in roc_auc_scores_expOOB:\n",
        "    print(f'expOOB ROC AUC for {dataset_name}: {score}')"
      ],
      "metadata": {
        "id": "OXNK3Mt7_pgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489ed73d-81da-4d8d-9509-a73fb87d5e73"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fetching diabetes from pmlb\n",
            "fetching heart from imodels\n",
            "fetching fico from imodels\n",
            "fetching credit_g from imodels\n",
            "Default ROC AUC for diabetes: 0.830925925925926\n",
            "Default ROC AUC for breast_cancer: 0.8282312925170068\n",
            "Default ROC AUC for heart: 0.9155844155844156\n",
            "Default ROC AUC for haberman: 0.6243686868686869\n",
            "Default ROC AUC for fico: 0.76640817245723\n",
            "Default ROC AUC for enhancer: 0.82537748709253\n",
            "Default ROC AUC for credit_g: 0.8158432503906718\n",
            "Default ROC AUC for juvenile_clean: 0.896691220867045\n",
            "====================================================================================================\n",
            "expOOB ROC AUC for diabetes: 0.8303703703703703\n",
            "expOOB ROC AUC for breast_cancer: 0.8282312925170068\n",
            "expOOB ROC AUC for heart: 0.9163059163059162\n",
            "expOOB ROC AUC for haberman: 0.6237373737373737\n",
            "expOOB ROC AUC for fico: 0.7662287431157722\n",
            "expOOB ROC AUC for enhancer: 0.8250545034839483\n",
            "expOOB ROC AUC for credit_g: 0.816925111191249\n",
            "expOOB ROC AUC for juvenile_clean: 0.8975020270153709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3cKwzh3REPFP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}